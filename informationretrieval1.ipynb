{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4629e0ae-b5fc-4342-80c3-f3077dae6644",
      "metadata": {},
      "source": [
        "<b>1ο Βήμα: Εισαγωγή Βιβλιοθηκών</b>\n",
        "\n",
        "Σε αυτό το βήμα, εισάγουμε τις απαραίτητες βιβλιοθήκες για web scraping,\n",
        "επεξεργασία φυσικής γλώσσας (NLP) και διαχείριση δεδομένων. Επιπλέον, κατεβάζουμε τους απαραίτητους πόρους από το NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "479233bb-217a-4331-91da-6b384e703247",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jimdr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\jimdr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\jimdr\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wikipedia\n",
        "import requests\n",
        "import nltk\n",
        "import string\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import FreqDist\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Λήφη απαραίτητων πόρων\n",
        "wikipedia.set_lang(\"en\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2ef0aa-9e0d-427e-91ea-debb8cf02611",
      "metadata": {},
      "source": [
        "<b>2ο Βήμα: Ορισμός URL</b>\n",
        "\n",
        "Αποθηκεύουμε σε μια λίστα τα URL της Wikipedia που περιέχουν τα άρθρα που θέλουμε να\n",
        "επεξεργαστούμε. Τα URL αυτά αντιστοιχούν σε διάφορα θέματα σχετικά με φαγητά και\n",
        "συνταγές."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a114eeaa-9ea0-4934-ba3d-860258d06345",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Λίστα URL των άρθρων από τη Wikipedia\n",
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Tiropita\",\n",
        "    \"https://en.wikipedia.org/wiki/Peinirli\", \n",
        "    \"https://en.wikipedia.org/wiki/Pizza\",\n",
        "    \"https://en.wikipedia.org/wiki/Spaghetti\",\n",
        "    \"https://en.wikipedia.org/wiki/Bread\",\n",
        "    \"https://en.wikipedia.org/wiki/Moussaka\",\n",
        "    \"https://en.wikipedia.org/wiki/Pastitsio\",\n",
        "    \"https://en.wikipedia.org/wiki/Soup\",\n",
        "    \"https://en.wikipedia.org/wiki/Galaktoboureko\",\n",
        "    \"https://en.wikipedia.org/wiki/Bougatsa\",\n",
        "    \"https://en.wikipedia.org/wiki/Savory_spinach_pie\",\n",
        "    \"https://en.wikipedia.org/wiki/Gigantes_plaki\",\n",
        "    \"https://en.wikipedia.org/wiki/Fasolada\",\n",
        "    \"https://en.wikipedia.org/wiki/Lokma\",\n",
        "    \"https://en.wikipedia.org/wiki/Snails_as_food\",\n",
        "    \"https://en.wikipedia.org/wiki/Pumpkin_pie\",\n",
        "    \"https://en.wikipedia.org/wiki/Souvlaki\",\n",
        "    \"https://en.wikipedia.org/wiki/Doughnut\",\n",
        "    \"https://en.wikipedia.org/wiki/Fried_egg\",\n",
        "    \"https://en.wikipedia.org/wiki/Omelette\",\n",
        "    \"https://en.wikipedia.org/wiki/Strapatsada\",\n",
        "    \"https://en.wikipedia.org/wiki/Scrambled_eggs\",\n",
        "    \"https://en.wikipedia.org/wiki/Poached_egg\",\n",
        "    \"https://en.wikipedia.org/wiki/Milk_pie\",\n",
        "    \"https://en.wikipedia.org/wiki/Melktert\",\n",
        "    \"https://en.wikipedia.org/wiki/Fit-fit\",\n",
        "    \"https://en.wikipedia.org/wiki/Cachupa\",\n",
        "    \"https://en.wikipedia.org/wiki/Kebab\",\n",
        "    \"https://en.wikipedia.org/wiki/Koshary\",\n",
        "    \"https://en.wikipedia.org/wiki/Mombar\",\n",
        "    \"https://en.wikipedia.org/wiki/Hamburger\",\n",
        "    \"https://en.wikipedia.org/wiki/Hot_dog\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89a52ae0-82ec-49fc-baa5-33b2f29a12ef",
      "metadata": {},
      "source": [
        "<b>3ο Βήμα: Ανάκτηση και Επεξεργασία Άρθρων</b>\n",
        "\n",
        "Σε αυτό το βήμα, ανακτουμε το περιεχόμενο των άρθρων από τη Wikipedia.\n",
        "Χρησιμοποιούμε τη βιβλιοθήκη BeautifulSoup για την\n",
        "ανάλυση του HTML περιεχομένου, αφαιρούμε τους συνδέσμους και τις έντονες\n",
        "γραμματοσειρές και εξάγουμε το κυρίως κείμενο από κάθε άρθρο σε ένα αρχείο JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a7529232-7bd7-440e-bb23-4f7d1d707a49",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Articles fetched and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Αρχικοποίηση λίστας για αποθήκευση άρθρων\n",
        "articles = []\n",
        "\n",
        "# Ανάκτηση άρθρων από κάθε URL\n",
        "for url in urls:\n",
        "    response = requests.get(url) # Κατεβάζουμε το περιεχόμενο του URL\n",
        "    response.encoding = 'utf-8' # Ρύθμιση encoding\n",
        "    soup = BeautifulSoup(response.text, 'html.parser') # Ανάλυση του HTML\n",
        "\n",
        "    # Αφαίρεση ετικετών HTML όπως <a>, <b>, <strong>\n",
        "    for a in soup.find_all('a'):\n",
        "        a.unwrap()\n",
        "    for bold in soup.find_all(['b', 'strong']):\n",
        "        bold.unwrap()\n",
        "\n",
        "    # Εξαγωγή κειμένου από παραγράφους\n",
        "    paragraphs = soup.find_all('p')\n",
        "    parsed_paragraphs = [p.get_text(separator=\" \", strip=True) for p in paragraphs]\n",
        "    articles.append(\" \".join(parsed_paragraphs))\n",
        "\n",
        "# Αποθήκευση άρθρων σε αρχείο JSON\n",
        "output_filename = \"articles.json\"\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Articles fetched and saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4309a2c6-98d9-42b2-982e-261e2a5b0eb9",
      "metadata": {},
      "source": [
        "<b>4ο Βήμα: Δημιουργία Αντεστραμμένου Ευρετηρίου</b>\n",
        "\n",
        "Σε αυτό το βήμα φορτώνουμε τα επεξεργασμένα άρθρα και δημιουργούμε το αντεστραμμένο ευρετήριο,\n",
        "το οποίο συνδέει κάθε όρο με τη λίστα των εγγράφων στα οποία εμφανίζεται."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdcc662d-4763-4626-9512-664c3213bcbc",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverted index created and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Φόρτωση άρθρων από το αρχείο\n",
        "with open(output_filename, 'r', encoding='utf-8') as f:\n",
        "    articles = json.load(f)\n",
        "\n",
        "# Αρχικοποίηση εργαλείων του NLTK\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Δημιουργία αντεστραμμένου ευρετηρίου \n",
        "inverted_index = {}\n",
        "for doc_id, document in enumerate(articles):\n",
        "    tokens = nltk.word_tokenize(document.lower()) # Διαχωρισμός σε λέξεις (tokens)\n",
        "    for token in tokens:\n",
        "        token = lemmatizer.lemmatize(token) # lemmatization\n",
        "        if token not in stopwords and token not in string.punctuation: # Φιλτράρισμα λέξεων\n",
        "            if token not in inverted_index:\n",
        "                inverted_index[token] = []     \n",
        "            if doc_id not in inverted_index[token]:    \n",
        "                inverted_index[token].append(doc_id)\n",
        "\n",
        "# Αποθήκευση του αντεστραμμένου ευρετηρίου\n",
        "inverted_index_filename = \"inverted_index.json\"\n",
        "with open(inverted_index_filename, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(inverted_index, json_file, indent=4)\n",
        "\n",
        "print(\"Inverted index created and saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedebc43-875e-4abc-a939-06288b60af3d",
      "metadata": {},
      "source": [
        "<b>5ο Βήμα: Επεξεργασία Ερωτήματος Χρήστη</b>\n",
        "\n",
        "Σε αυτό το βήμα, επεξεργαζόμαστε την ερώτηση που κάνει ο χρήστης.\n",
        "Η ερώτηση χωρίζεται σε tokens,\n",
        "τα κεφαλαία γράμματα μετατρέπονται σε πεζά, γίνεται lemmatization,\n",
        "αφαιρούνται τα stopwords και τα σημεία στίξης."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5df53059-2df2-4362-b33c-7ea548613935",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed query tokens: ['cheese']\n"
          ]
        }
      ],
      "source": [
        "# Εισαγωγή ερώτηματος χρήστη\n",
        "query= input(\"type a query: \")\n",
        "\n",
        "# Επεξεργασία του ερωτήματος\n",
        "query_tokens=nltk.word_tokenize(query.lower())\n",
        "query_tokens = [lemmatizer.lemmatize(token) for token in query_tokens if token not in stopwords and token not in string.punctuation]\n",
        "\n",
        "print(\"Processed query tokens:\", query_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237586c3-f640-4b9d-ac1e-c303a1ce86d2",
      "metadata": {},
      "source": [
        "<b>6ο Βήμα: Εκτέλεση Αναζητήσεων AND/OR/NOT </b>\n",
        "\n",
        "Σε αυτό το βήμα, εκτελούνται τρεις τύποι αναζητήσεων με βάση την ερώτηση:\n",
        "1. Αναζήτηση AND, όπου επιστρέφονται τα έγγραφα που περιέχουν όλους τους όρους της ερώτησης.\n",
        "2. Αναζήτηση OR, όπου επιστρέφονται τα έγγραφα που περιέχουν τουλάχιστον έναν από τους όρους της ερώτησης.\n",
        "3. Αναζήτηση NOT, όπου επιστρέφονται τα έγγραφα που δεν περιέχουν κανέναν από τους όρους της ερώτησης."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5aea3cce-3d33-4aec-bf76-718e0223ac85",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 .  Tiropita or tyropita ...\n",
            "2 .  The peinirli (or pen ...\n",
            "3 .   Pizza [ a ] [ 1 ] i ...\n",
            "4 .   Spaghetti ( Italian ...\n",
            "5 .   Bread is a staple f ...\n",
            "6 .   Moussaka ( / m uː ˈ ...\n",
            "7 .  Pastitsio ( Greek :  ...\n",
            "8 .    Soup is a primaril ...\n",
            "9 .   Bougatsa ( Greek :  ...\n",
            "10 .  Savory spinach pie i ...\n",
            "11 .  Gigantes plaki ( Gre ...\n",
            "12 .  Lokma , is a dessert ...\n",
            "13 .   A doughnut (sometim ...\n",
            "14 .   A fried egg is a co ...\n",
            "15 .  An omelette (sometim ...\n",
            "16 .  Strapatsada (Greek:  ...\n",
            "17 .   Scrambled eggs is a ...\n",
            "18 .    Kebab ( UK : / k ɪ ...\n",
            "19 .   A hamburger , or si ...\n",
            "20 .    A hot dog [ 1 ] [  ...\n",
            "21 .  Galaktoboureko ( Gre ...\n",
            "22 .  Fasolada ( Greek : φ ...\n",
            "23 .  Snails are eaten by  ...\n",
            "24 .  Pumpkin pie is a des ...\n",
            "25 .  Souvlaki ( Greek : σ ...\n",
            "26 .   A poached egg is an ...\n",
            "27 .  Pie susu ( Indonesia ...\n",
            "28 .   Melktert ( / ˈ m ɛ  ...\n",
            "29 .  Fit-fit or fir-fir ( ...\n",
            "30 .  Cachupa ( Portuguese ...\n",
            "31 .  Koshary , kushari or ...\n",
            "32 .   Mombar (in ِ Egypti ...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "AND docs: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 17, 18, 19, 20, 21, 27, 30, 31]\n",
            "OR docs: []\n",
            "NOT docs: [8, 12, 14, 15, 16, 22, 23, 24, 25, 26, 28, 29]\n"
          ]
        }
      ],
      "source": [
        "and_docs = []\n",
        "or_docs = []\n",
        "not_docs = []\n",
        "\n",
        "for doc_id, document in enumerate(articles):\n",
        "    count=0\n",
        "    for token in query_tokens:\n",
        "        if token in inverted_index:\n",
        "            if doc_id in inverted_index[token]:\n",
        "                count=count+1        \n",
        "    if count==len(query_tokens):\n",
        "        and_docs.append(doc_id) # Έγγραφα που περιέχουν όλους τους όρους\n",
        "    elif count>0:\n",
        "        or_docs.append(doc_id) # Έγγραφα που περιέχουν μερικούς από τους όρους\n",
        "    else:\n",
        "        not_docs.append(doc_id) # Έγγραφα που δεν περιέχουν κανέναν από τους όρους\n",
        "\n",
        "j=1\n",
        "\n",
        "for i in and_docs:\n",
        "    print(j, \". \", articles[i][0:20], \"...\")\n",
        "    j=j+1\n",
        "    \n",
        "for i in or_docs:\n",
        "    print(j, \". \", articles[i][0:20], \"...\")\n",
        "    j=j+1\n",
        "        \n",
        "for i in not_docs:\n",
        "    print(j, \". \", articles[i][0:20], \"...\")\n",
        "    j=j+1\n",
        "    \n",
        "print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "print(\"AND docs:\", and_docs)\n",
        "print(\"OR docs:\", or_docs)\n",
        "print(\"NOT docs:\", not_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f5ce6b-96d0-4ad6-b71b-a2f755e4b6e6",
      "metadata": {},
      "source": [
        "<b>7ο Βήμα: Υπολογισμός TF-IDF </b>\n",
        "\n",
        "Σε αυτό το βήμα, υπολογίζονται τα scores του TF-IDF για τους όρους\n",
        "της ερώτησης σε κάθε έγγραφο. Με βάση τα scores ταξινομούνται τα έγγραφα.\n",
        "Ο υπολογισμός του TF-IDF, αξιολογεί την σημασία κάθε όρου του ερωτήματος χρήστη σε ένα έγγραφο. \n",
        "Το TF αποτελεί τη συχνότητα ενός όρου σε ένα έγγραφο.\n",
        "Το IDF αφορά την μοναδικότητα ενός όρου σε ολόκληρη την συλλογή των έγγραφων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "34e9801b-22a5-4d3b-9c57-0f1c9526ac57",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranked documents: [(1, 0.030128437772162536), (0, 0.01532620530149138), (10, 0.012702800790425288), (6, 0.00982097135737358), (9, 0.0054440574816108376), (20, 0.0046535012796607485), (21, 0.0038108402371275865), (2, 0.0034793606113194494), (11, 0.0029747065142135165), (19, 0.0024867916891308764), (30, 0.0018773326132497288), (5, 0.0018181958578171592), (31, 0.0015153260480786747), (3, 0.0014551195951880362), (7, 0.0011533831392533391), (18, 0.0009989450143373763), (13, 0.0007939250494015805), (27, 0.00040188424903440413), (17, 0.0002849082416118826), (4, 0.00018967055256082955), (8, 0.0), (12, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (28, 0.0), (29, 0.0)]\n",
            "Ranked Documents:\n",
            "1 .   The peinirli (or pen - Score: 0.0301\n",
            "2 .   Tiropita or tyropita - Score: 0.0153\n",
            "3 .   Savory spinach pie i - Score: 0.0127\n",
            "4 .   Pastitsio ( Greek :  - Score: 0.0098\n",
            "5 .    Bougatsa ( Greek :  - Score: 0.0054\n",
            "6 .   Strapatsada (Greek:  - Score: 0.0047\n",
            "7 .    Scrambled eggs is a - Score: 0.0038\n",
            "8 .    Pizza [ a ] [ 1 ] i - Score: 0.0035\n",
            "9 .   Gigantes plaki ( Gre - Score: 0.0030\n",
            "10 .   An omelette (sometim - Score: 0.0025\n",
            "11 .    A hamburger , or si - Score: 0.0019\n",
            "12 .    Moussaka ( / m uː ˈ - Score: 0.0018\n",
            "13 .     A hot dog [ 1 ] [  - Score: 0.0015\n",
            "14 .    Spaghetti ( Italian - Score: 0.0015\n",
            "15 .     Soup is a primaril - Score: 0.0012\n",
            "16 .    A fried egg is a co - Score: 0.0010\n",
            "17 .   Lokma , is a dessert - Score: 0.0008\n",
            "18 .     Kebab ( UK : / k ɪ - Score: 0.0004\n",
            "19 .    A doughnut (sometim - Score: 0.0003\n",
            "20 .    Bread is a staple f - Score: 0.0002\n",
            "21 .   Galaktoboureko ( Gre - Score: 0.0000\n",
            "22 .   Fasolada ( Greek : φ - Score: 0.0000\n",
            "23 .   Snails are eaten by  - Score: 0.0000\n",
            "24 .   Pumpkin pie is a des - Score: 0.0000\n",
            "25 .   Souvlaki ( Greek : σ - Score: 0.0000\n",
            "26 .    A poached egg is an - Score: 0.0000\n",
            "27 .   Pie susu ( Indonesia - Score: 0.0000\n",
            "28 .    Melktert ( / ˈ m ɛ  - Score: 0.0000\n",
            "29 .   Fit-fit or fir-fir ( - Score: 0.0000\n",
            "30 .   Cachupa ( Portuguese - Score: 0.0000\n",
            "31 .   Koshary , kushari or - Score: 0.0000\n",
            "32 .    Mombar (in ِ Egypti - Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Υπολογισμός συχνότητας όρων (TF)\n",
        "doc_tokens = []\n",
        "for document in articles:\n",
        "    tokens = nltk.word_tokenize(document.lower())\n",
        "    lem_tokens = [\n",
        "        lemmatizer.lemmatize(token) for token in tokens if token not in stopwords and token not in string.punctuation\n",
        "    ]\n",
        "    doc_tokens.append(lem_tokens) \n",
        "    \n",
        "tf_list = []\n",
        "for tokens in doc_tokens:\n",
        "    freq_dist = FreqDist(tokens)\n",
        "    total_terms = len(tokens)\n",
        "    tf = {term: freq / total_terms for term, freq in freq_dist.items()}\n",
        "    tf_list.append(tf)\n",
        "\n",
        "# Υπολογισμός αντίστροφης συχνότητας όρων (IDF)\n",
        "total_docs = len(articles)\n",
        "idf = {}\n",
        "for term, doc_ids in inverted_index.items():\n",
        "    idf[term] = math.log(total_docs / len(doc_ids))\n",
        "    \n",
        "# Υπολογισμός scores TF-IDF\n",
        "scores = defaultdict(float)\n",
        "for doc_id, tokens in enumerate(doc_tokens):\n",
        "    tf = tf_list[doc_id]\n",
        "    tf_idf = {term: tf.get(term, 0) * idf.get(term, 0) for term in tf}\n",
        "    for token in query_tokens:\n",
        "        scores[doc_id] += tf_idf.get(token, 0)\n",
        "\n",
        "# Κατάταξη εγγράφων με βάση τα score\n",
        "ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Ranked documents:\", ranked_docs)\n",
        "\n",
        "j=1\n",
        "print(\"Ranked Documents:\")\n",
        "for doc_id, score in ranked_docs:\n",
        "    print(j,\". \", f\" {articles[doc_id][0:20]} - Score: {score:.4f}\")\n",
        "    j=j+1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "864dec8e-685d-4abc-98ad-3b1ea810df26",
      "metadata": {},
      "source": [
        "<b>8ο Βήμα: Αξιολόγηση Απόδοσης</b>\n",
        "\n",
        "Σε αυτό το βήμα αξιολογούμε την απόδοση του συτήματος ανάκτησης πληροφοριών που κατασκευάσαμε. Έτσι, δημιουργούμε ένα σύνολο ερωτημάτων, όπου για κάθε ένα ερώτημα υπολογίζουμε: \n",
        "1. Precision (Ακρίβεια), δηλαδή το ποσοστό των σχετικών εγγράφων μεταξύ των εγγράφων που ανακτήθηκαν.\n",
        "2. Recall (Ανάκληση), δηλαδή το ποσοστό των σχετικών εγγράφων που ανακτήθηκαν επιτυχώς.\n",
        "3. F1-Score, δηλαδή τον αρμονικό μέσο της ακρίβειας και της ανάκλησης\n",
        "4. Μέση Ακρίβεια (MAP), τον μέσο όρο της ακρίβειας"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "abf91cc2-0c1d-4b24-99ea-e66899565034",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "system efficiency for the query:  what is pizza?\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Mean Average Precision (MAP): 1.0000\n",
            "\n",
            "system efficiency for the query:  food with sugar and flour\n",
            "Precision: 1.0000\n",
            "Recall: 0.6667\n",
            "F1 Score: 0.8000\n",
            "Mean Average Precision (MAP): 0.6979\n",
            "\n",
            "system efficiency for the query:  food with eggs\n",
            "Precision: 1.0000\n",
            "Recall: 0.7857\n",
            "F1 Score: 0.8800\n",
            "Mean Average Precision (MAP): 0.8795\n",
            "\n",
            "system efficiency for the query:  food with cinnamon\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Mean Average Precision (MAP): 1.0000\n",
            "\n",
            "system efficiency for the query:  which food is sweet\n",
            "Precision: 1.0000\n",
            "Recall: 0.7143\n",
            "F1 Score: 0.8333\n",
            "Mean Average Precision (MAP): 0.7768\n",
            "\n",
            "system efficiency for the query:  what food contains cheese\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Mean Average Precision (MAP): 1.0000\n"
          ]
        }
      ],
      "source": [
        "queries = [\n",
        "    \"what is pizza?\", \n",
        "    \"food with sugar and flour\", \n",
        "    \"food with eggs\",\n",
        "    \"food with cinnamon\",\n",
        "    \"which food is sweet\",\n",
        "    \"what food contains cheese\"\n",
        "]\n",
        "\n",
        "relevant_docs = [\n",
        "        [2],\n",
        "        [4,8,9],\n",
        "        [0,5,6,8,9,10,20,21,22,23,24,25,28,29],\n",
        "        [9,15],\n",
        "        [8,9,12,15,17,22,23],\n",
        "        [0,5,6,9,10,18]\n",
        "    ]\n",
        "\n",
        "for q in range(6):\n",
        "    query_tokens = nltk.word_tokenize(queries[q].lower())\n",
        "    query_tokens = [lemmatizer.lemmatize(token) for token in query_tokens if token not in stopwords and token not in string.punctuation]\n",
        "\n",
        "    doc_tokens = []\n",
        "    for document in articles:\n",
        "        tokens = nltk.word_tokenize(document.lower())\n",
        "        lem_tokens = [\n",
        "            lemmatizer.lemmatize(token) for token in tokens if token not in stopwords and token not in string.punctuation\n",
        "        ]\n",
        "        doc_tokens.append(lem_tokens) \n",
        "    \n",
        "    # Υπολογισμός συχνότητας όρων (TF)\n",
        "    tf_list = []\n",
        "    for tokens in doc_tokens:\n",
        "        freq_dist = FreqDist(tokens)\n",
        "        total_terms = len(tokens)\n",
        "        tf = {term: freq / total_terms for term, freq in freq_dist.items()}\n",
        "        tf_list.append(tf)\n",
        "    \n",
        "    # Υπολογισμός αντίστροφης συχνότητας όρων (IDF)\n",
        "    total_docs = len(articles)\n",
        "    idf = {}\n",
        "    for term, doc_ids in inverted_index.items():\n",
        "        idf[term] = math.log(total_docs / len(doc_ids))\n",
        "        \n",
        "    # Υπολογισμός scores TF-IDF\n",
        "    scores = defaultdict(float)\n",
        "    for doc_id, tokens in enumerate(doc_tokens):\n",
        "        tf = tf_list[doc_id]\n",
        "        tf_idf = {term: tf.get(term, 0) * idf.get(term, 0) for term in tf}\n",
        "        for token in query_tokens:\n",
        "            scores[doc_id] += tf_idf.get(token, 0)\n",
        "        \n",
        "        \n",
        "    print(\"\\nsystem efficiency for the query: \", queries[q])\n",
        "    \n",
        "    binary_relevance = [1 if i in relevant_docs[q] and scores[i]>0.0 else 0 for i in range(len(articles))]\n",
        "    true_relevance = [1 if i in relevant_docs[q] else 0 for i in range(len(articles))]\n",
        "    precision = precision_score(true_relevance, binary_relevance, zero_division=0)\n",
        "    recall = recall_score(true_relevance, binary_relevance, zero_division=0)\n",
        "    f1 = f1_score(true_relevance, binary_relevance, zero_division=0)\n",
        "    average_precision = average_precision_score(true_relevance, binary_relevance)\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Mean Average Precision (MAP): {average_precision:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
